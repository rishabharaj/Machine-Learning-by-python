# Gradient Descent and Backpropagation

## Introduction

Gradient descent and backpropagation are fundamental concepts in training neural networks. They work together to optimize the network's parameters (weights and biases) to minimize the prediction error.

## Gradient Descent

### What is Gradient Descent?
- Optimization algorithm
- Finds minimum of a function
- Iteratively adjusts parameters
- Uses gradient (slope) information

### Types of Gradient Descent

#### 1. Batch Gradient Descent
- Uses entire dataset
- Computationally expensive
- Stable convergence
- Memory intensive

#### 2. Stochastic Gradient Descent (SGD)
- Uses single sample
- Faster updates
- Noisy convergence
- Memory efficient

#### 3. Mini-batch Gradient Descent
- Compromise between batch and SGD
- Uses small batches
- Most commonly used
- Good balance of speed and stability

### Learning Rate
- Controls step size
- Critical hyperparameter
- Too high: overshooting
- Too low: slow convergence

## Backpropagation

### What is Backpropagation?
- Algorithm for training neural networks
- Calculates gradients efficiently
- Chain rule of calculus
- Updates weights layer by layer

### The Process

#### 1. Forward Pass
- Input flows through network
- Calculate predictions
- Compute loss

#### 2. Backward Pass
- Calculate gradients
- Update weights
- Propagate errors backward

### Chain Rule in Backpropagation
- Decomposes complex derivatives
- Breaks down into simple steps
- Enables efficient computation
- Handles multiple layers

## Loss Functions

### Common Loss Functions

#### 1. Mean Squared Error (MSE)
- For regression tasks
- Measures average squared difference
- Sensitive to outliers
- Differentiable

#### 2. Cross-Entropy Loss
- For classification tasks
- Measures probability distribution difference
- Handles multiple classes
- Works well with softmax

### Regularization

#### 1. L1 Regularization (Lasso)
- Adds absolute value of weights
- Promotes sparsity
- Feature selection

#### 2. L2 Regularization (Ridge)
- Adds squared weights
- Prevents large weights
- Improves generalization

## Optimization Techniques

### 1. Momentum
- Accumulates past gradients
- Reduces oscillations
- Speeds up convergence
- Helps escape local minima

### 2. RMSprop
- Adapts learning rate
- Uses moving average
- Handles sparse gradients
- Good for online learning

### 3. Adam
- Combines momentum and RMSprop
- Adaptive learning rates
- Most popular optimizer
- Works well in practice

## Common Challenges

### 1. Vanishing Gradients
- Gradients become very small
- Early layers learn slowly
- Common in deep networks
- Solutions: ReLU, BatchNorm

### 2. Exploding Gradients
- Gradients become very large
- Unstable training
- Can cause NaN values
- Solutions: Gradient clipping

### 3. Local Minima
- Gets stuck in suboptimal solutions
- Multiple local minima
- Hard to escape
- Solutions: Momentum, different initializations

## Best Practices

### 1. Initialization
- Random initialization
- Xavier/Glorot initialization
- He initialization
- Avoid zero initialization

### 2. Learning Rate
- Start with small value
- Use learning rate schedules
- Monitor loss curve
- Adjust based on performance

### 3. Batch Size
- Balance speed and stability
- Power of 2 (32, 64, 128)
- Consider memory constraints
- Monitor convergence

## Next Steps

In the following lessons, we will:
1. Set up deep learning frameworks
2. Implement these concepts in code
3. Train our first neural network 